{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eFqBUc_s09v",
        "outputId": "b44f15f6-6b3f-4215-a7e6-6a3f4735bb04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.7/dist-packages (0.2.8)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (6.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.9.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.7)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.3.1)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (6.0.10)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.1->newspaper3k) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.8.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "   #installing and importing necessary libraries \n",
        "!pip install newspaper3k\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from newspaper import Article\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string, nltk\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from textblob import TextBlob\n",
        "from nltk import FreqDist\n",
        "from wordcloud import WordCloud\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stopwords = list(nltk.corpus.stopwords.words('english'))\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install gensim\n",
        "import gensim as gm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mLPzMgqHndZ",
        "outputId": "5243a436-1532-4b79-dd92-512bc835b8d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       ¬°Ya falta poco para llenar el √°lbum! üò±üôåüá™üá®\\n\\n¬ø...\n",
              "1       Llenando su primer √°lbum! @fifaworldcup #fifaw...\n",
              "2       @hardfootyvideos He won't be no longer since @...\n",
              "3       @MaGeLaNe72 @Zaubermaus57 Garantiert nicht. Mi...\n",
              "4       ‡¥ñ‡¥§‡µç‡¥§‡¥±‡¥ø‡¥≤‡µÜ ‡¥®‡µã‡¥ü‡µç‡¥ü‡¥™‡µç‡¥™‡µÅ‡¥≥‡µç‡¥≥‡¥ø‡¥ï‡¥≥‡µç‚Äç: ‡¥ú‡¥æ‡¥®‡µç‚Äç ‡¥¨‡µÜ‡¥°‡µç‡¥®‡¥æ‡¥∞‡µç‚Äç‡¥ï‡µç‡¥ï...\n",
              "                              ...                        \n",
              "4544    Qatar continues to evade accountability for th...\n",
              "4545    LGBTQ Football Fans Told to Use ‚ÄòCommon Sense‚Äô...\n",
              "4546    How many teams are required to play an officia...\n",
              "4547    More than 270 violations of #Qatar summer work...\n",
              "4548    WAGE GULF #Qatar #WorldCup shame: Builders are...\n",
              "Name: Tweet, Length: 4549, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#reading raw text\n",
        "df = pd.read_csv(r'/content/Twitter Data.csv')\n",
        "data = df.Tweet\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqaZGavtq1s6"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBCzzKEdoM8n"
      },
      "outputs": [],
      "source": [
        "# Data Pre-processing \n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def process(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = data[i].lower()\n",
        "    data[i] = re.sub(r\"http\\S+\", '', str(data[i]))\n",
        "    data[i] = \"\".join([a for a in data[i] if a not in string.punctuation]) #remove punctuations\n",
        "    data[i] = \"\".join([a for a in data[i] if not a.isdigit()]) #remove digits\n",
        "    data[i] = TextBlob(data[i])\n",
        "    data[i] = data[i].correct()\n",
        "    data[i] = \" \".join([a for a in data[i].split() if not a in stopwords]) #stopwords\n",
        "    data[i] = \" \".join([lemma.lemmatize(word) for word in data[i].split()]) #lemmatization\n",
        "    \n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNZU633JoQf9",
        "outputId": "2db0b63f-ffde-467d-bd1a-f49b101b0d4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm6kUMB7tU-i",
        "outputId": "05c622f5-e5fb-473d-fe69-e1c8b57fad19"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n"
          ]
        }
      ],
      "source": [
        "p = process(df.Tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLm4akBOoWhQ"
      },
      "outputs": [],
      "source": [
        "df1 = pd.DataFrame(list(zip(p)), columns =['Tweet'])\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hioXfg5ofOZ"
      },
      "outputs": [],
      "source": [
        "entire_corpus_text = str( )\n",
        "for i in p: entire_corpus_text+=str(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTRl_nzdol6O"
      },
      "outputs": [],
      "source": [
        "!pip install seaborn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssYSlNadopQ6"
      },
      "outputs": [],
      "source": [
        "# Identifying most common words \n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "## Creating FreqDist for whole BoW, keeping the 50 most common tokens\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "all_fdist = FreqDist(word_tokenize(entire_corpus_text)).most_common(50)\n",
        "\n",
        "## Conversion to Pandas series via Python Dictionary for easier plotting\n",
        "all_fdist = pd.Series(dict(all_fdist))\n",
        "\n",
        "## Setting figure, ax into variables\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "\n",
        "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
        "all_plot = sns.barplot(x=all_fdist.index, y=all_fdist.values, ax=ax)\n",
        "plt.xticks(rotation=90);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjvWHAGtotaS"
      },
      "outputs": [],
      "source": [
        "# second iteration for removing special characters / addional words etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0aH57kq26KB"
      },
      "outputs": [],
      "source": [
        "# Add stop words \n",
        "new_stopwords = ['\\'','told', 'said','s','u','also','would','day','take','xi','k''\\'','told', 'said','s','u','also','would','day','take','xi','ok','0','phone','actually','abuse','amazon','alter','apart','aside','buy','bought','choose','compare','customer','customersexcept','department','donna','dont','exactly','exchange',\n",
        " \n",
        " 'law','life',\n",
        " 'lifefast',\n",
        " 'like',\n",
        " 'liked','logic',\n",
        " ]\n",
        "stopwords.extend(new_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjAwYIkUo3xu"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "\n",
        "def process(data):\n",
        "  for i in range(len(data)):\n",
        "    data[i] = data[i].lower()\n",
        "    data[i] = re.sub(r\"http\\S+\", '', str(data[i]))\n",
        "    data[i] = re.sub(\"[!@#$%^&*()[]{};:,./<>?\\|`~-=_+ ‚Äú]\", '', str(data[i]))\n",
        "    data[i] = re.sub(\"‚Äú\", '', str(data[i]))\n",
        "    data[i] = re.sub(\"‚Äù\", '', str(data[i]))\n",
        "    data[i] = re.sub(\"‚Äô\", '', str(data[i]))\n",
        "    data[i] = \"\".join([a for a in data[i] if a not in string.punctuation]) #remove punctuations\n",
        "    data[i] = \"\".join([a for a in data[i] if not a.isdigit()]) #remove digits\n",
        "    data[i] = \" \".join([a for a in data[i].split() if not a in stopwords]) #stopwords\n",
        "     \n",
        "    \n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdojieeao7Ef"
      },
      "outputs": [],
      "source": [
        "p1 = process(df1.Text)\n",
        "entire_corpus_text1 = str( )\n",
        "for i in p1: entire_corpus_text1+=str(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkGvAL2cv_o3"
      },
      "outputs": [],
      "source": [
        "p1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIfTS4jxpQgS"
      },
      "outputs": [],
      "source": [
        "#Finding most common words with frequency\n",
        "FreqDist(word_tokenize(entire_corpus_text1)).most_common(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6A3AsxWpV0X"
      },
      "outputs": [],
      "source": [
        "# Identifying most common words \n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "## Creating FreqDist for whole BoW, keeping the 50 most common tokens\n",
        "all_fdist1 = FreqDist(word_tokenize(entire_corpus_text1)).most_common(50)\n",
        "\n",
        "## Conversion to Pandas series via Python Dictionary for easier plotting\n",
        "all_fdist1 = pd.Series(dict(all_fdist1))\n",
        "\n",
        "## Setting figure, ax into variables\n",
        "fig, ax = plt.subplots(figsize=(18,10))\n",
        "\n",
        "## Seaborn plotting using Pandas attributes + xtick rotation for ease of viewing\n",
        "all_plot1 = sns.barplot(x=all_fdist1.index, y=all_fdist1.values, ax=ax)\n",
        "plt.xticks(rotation=90);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD8quLCvpYqg"
      },
      "outputs": [],
      "source": [
        "#Word Cloud\n",
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TAk609opcuH"
      },
      "outputs": [],
      "source": [
        "final_text1=\"\".join(df1[\"Text\"])\n",
        "len(final_text1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L96HvfgZpjYQ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (15, 15), facecolor = None) \n",
        "wc=WordCloud(colormap='Set2').generate(final_text1)\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXrah8cxwHh9"
      },
      "source": [
        "Topic Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etuWY727x-Do"
      },
      "outputs": [],
      "source": [
        "df12 = {\"Reviews\" : p1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXjpXWfNplM4"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(df12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5-HLmeXwOL7"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX6GR5VGwOIf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "cv= CountVectorizer(max_df=0.75, min_df=1, stop_words='english')\n",
        "#max-df : ignore terms that have max or min\n",
        "dtm=cv.fit_transform(data['Reviews'])\n",
        "dtm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkERNBNywOFr"
      },
      "outputs": [],
      "source": [
        "dtm.A "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFvhSOf9wOCn"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "LDA = LatentDirichletAllocation(n_components = 5, random_state=20)\n",
        "\n",
        "LDA.fit(dtm)  \n",
        "w1 =LDA.fit_transform(dtm) \n",
        "h1= LDA.components_ # Printing Topics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw1qUp2-wN_v"
      },
      "outputs": [],
      "source": [
        "w1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9NAOdeXwN8d"
      },
      "outputs": [],
      "source": [
        "w1[0].argmax() # article belongs to this topic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN4J9uUZwN5x"
      },
      "outputs": [],
      "source": [
        "w1[1].argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARtBdFgQwNep"
      },
      "outputs": [],
      "source": [
        "# print top words in each topic\n",
        "for index,topic in enumerate(LDA.components_):\n",
        "    print(f'The top 15 words for topic  #{index}')\n",
        "    print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0kAhDbEwNY5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "# get all unique words in the corpus  ; grab vocabulary \n",
        "vocab = cv.get_feature_names()\n",
        "# show document feature vectors ; Grab topic\n",
        "pd.DataFrame(h1, columns=vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Fsobm50EsD2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWDRgrgxwrSS"
      },
      "outputs": [],
      "source": [
        "# step 1\n",
        "len(cv.get_feature_names()) # grab vocab "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz7jzV0nwtoj"
      },
      "outputs": [],
      "source": [
        "cv.get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bv5IjtGwv_-"
      },
      "outputs": [],
      "source": [
        "#top_word_indices= single_topic.argsort()[-20:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ3ptRETwyvh"
      },
      "outputs": [],
      "source": [
        "#for index in top_word_indices:\n",
        "#    print(cv.get_feature_names()[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JbpoYkywysZ"
      },
      "outputs": [],
      "source": [
        "for index,topic in enumerate(LDA.components_):\n",
        "    print(f'The top 10 words for topic  #{index}')\n",
        "    print([cv.get_feature_names()[i] for i in topic.argsort()[-10:]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWjx4mfx5MLE"
      },
      "source": [
        "\n",
        "1.   Fast OS\n",
        "2.   Discharge quickly\n",
        "3.   Good processor\n",
        "4.   Build Quality\n",
        "5.   Heavy Games compatibility\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs6BEYQmvgd1"
      },
      "outputs": [],
      "source": [
        "topic_results= LDA.transform(dtm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WZ09N2nvgd1"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdqYC1gnvgd2"
      },
      "outputs": [],
      "source": [
        "topic_results.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh-co0JYvgd2"
      },
      "outputs": [],
      "source": [
        "data['Topic']=topic_results.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFkZQjWFvgd2"
      },
      "outputs": [],
      "source": [
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VtOBNp77QCI"
      },
      "outputs": [],
      "source": [
        "Thank You !!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}